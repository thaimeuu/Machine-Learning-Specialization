## `Objective optimization`

> In the earlier courses, courses one and two of the specialization, you saw a lot of supervised learning algorithms as taking training set posing a cost function. And then using grading descent or some other algorithms to optimize that cost function. It turns out that the K-means algorithm that you saw in the last video is also optimizing a specific cost function. Although the optimization algorithm that it uses to optimize that is not gradient descent is actually the algorithm that you already saw in the last video. 

![Alt text](<ref img/7.png>)

- The 1<sup>st</sup> step of the iteration minimizes the cost function by changing **c<sup>(i)</sup>** and keeping the **&mu;** fixed
- The 2<sup>nd</sup> step of the iteration minimizes the cost function by changing **&mu;** and keeping the **c<sup>(i)</sup>** fixed

![Alt text](<ref img/8.png>)