## `XGBoost`

> Over the years, machine learning researchers have come up with a lot of different ways to build decision trees and decision tree ensembles. Today by far the most commonly used way or implementation of decision tree ensembles or decision trees there's an algorithm called XGBoost.

## `Intuition`

> When sampling, instead of picking from all m examples of equal probability with one over m probability, let's make it more likely that we'll pick misclassified examples that the previously trained trees do poorly on. In training and education, there's an idea called deliberate practice. For example, if you're learning to play the piano and you're trying to master a piece on the piano rather than practicing the entire say five minute piece over and over, which is quite time consuming. If you instead play the piece and then focus your attention on just the parts of the piece that you aren't yet playing that well in practice those smaller parts over and over. Then that turns out to be a more efficient way for you to learn to play the piano well. And so this idea of boosting is similar. We're going to look at the decision trees, we've trained so far and look at what we're still not yet doing well on. And then when building the next decision tree, we're going to focus more attention on the examples that we're not yet doing well.

![Alt text](<ref img/8.png>)

- When creating the b<sup>th</sup> tree, we look at the misclassified examples from 1<sup>st</sup> to (b-1)<sup>th</sup> trees and assign these examples higher probabilities of getting picked.

![Alt text](<ref img/9.png>)

## `XGBoost (eXtreme Gradient Boosting)`

![Alt text](<ref img/10.png>)

![Alt text](<ref img/11.png>)