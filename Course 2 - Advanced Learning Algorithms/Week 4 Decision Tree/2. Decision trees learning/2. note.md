## `Choosing a split: Information Gain`

> When building a decision tree, the way we'll decide what feature to split on at a node will be based on **what choice of feature reduces entropy the most**. Reduces entropy or reduces impurity, or maximizes purity. In decision tree learning, **the reduction of entropy** is called **information gain**.

- Choosing a split:
  - The root node's entropy is H(0.5) = 1
  - We then calculate the **information gain** of all 3 possible splits and choose the split with **the highest information gain**.

![Alt text](<ref img/3.png>)

## `Information gain`

- **p<sub>1</sub>** is the fraction of example that equals to 1.
- **p<sub>1</sub><sup>left</sup>** is the fraction of example that equals to 1 in the left sub-branch.
- **w<sup>left</sup>** is the number of examples after the split in the left sub-branch over the number of examples before the split.

![Alt text](<ref img/4.png>)