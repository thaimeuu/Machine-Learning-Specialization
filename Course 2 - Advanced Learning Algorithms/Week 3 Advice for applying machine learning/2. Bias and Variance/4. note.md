## `Learning curves`

- **Learning curves** are indicators of how well your model is doing on a certain **amount of experience - the number of training examples**.
- In the example below, there are 2 learning curves:
  - **J<sub>cv</sub>** decreases when **m<sub>train</sub>** increases.
  - **J<sub>train</sub>** increases when **m<sub>train</sub>** increases.

![Alt text](<ref img/12.png>)

### `High bias learning curves`

- In case your model has a high bias, the **learning curves** should look like the image below.
- In detail:
  - **J<sub>train</sub>** flattens out after a certain amount of training examples. -> Increasing training examples won't help in the case of **high bias**.
  - **J<sub>cv</sub>**, similarly, comes down at first and gradually flattens out.
  - There is a big gap between **J<sub>train</sub>** and **the baseline level of performance** indicating a high bias problem.

![Alt text](<ref img/13.png>)

### `High variance learning curves`

- In case of a high variance:
  - there is a big gap between **J<sub>cv</sub>** and **J<sub>train</sub>**. However, as **m<sub>train</sub>** increases, this gap becomes smaller because the **learning curves** don't flatten out as quickly as in the case of high bias. -> **Increasing training examples** will help improve the model.
  - Sometimes, **J<sub>train</sub>** can be better/lower than **the baseline** indicating that the model does better than human.

![Alt text](<ref img/14.png>)

### `Note`

- **Learning curves** method isn't implemented that often because it is computationally expensive due to training a model many times with many sizes of training data.
- However, knowing this can help you visualize the problem that your model may have.