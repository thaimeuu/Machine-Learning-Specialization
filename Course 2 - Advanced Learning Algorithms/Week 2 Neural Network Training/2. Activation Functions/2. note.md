## `Choosing activation functions`

### `How to choose activation function for OUTPUT layer`

![Alt text](<ref img/2.1.png>)

It depends on the ground truth's characteristics:
- y = 0/1 --> Sigmoid
- y = +/- --> Linear (Stock Trend)
- y = 0 or + --> ReLU (House price)

### `How to choose activation function for HIDDEN layer`

![Alt text](<ref img/2.2.png>)

Most common choice is **ReLU** because it:
- computes faster
- makes **gradient descent** more efficient