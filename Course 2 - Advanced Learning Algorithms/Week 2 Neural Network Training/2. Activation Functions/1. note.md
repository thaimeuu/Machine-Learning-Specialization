## `Alternatives to the sigmoid activation`

### `Most common Activation functions in Neural Networks`

![Alt text](<ref img/1.1.png>)

- Linear
- Sigmoid
- ReLU (Rectified Linear Unit)