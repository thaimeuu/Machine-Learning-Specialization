## `Recap`

`Forward propagation, computational graphs, Backward propagation`

- Forward Propagation:

    - Definition: Forward propagation is the process of transmitting input data through the neural network to generate an output. It involves the flow of information from the input layer, through the hidden layers, to the output layer.
    - Steps:
        - Each node (neuron) in a layer receives input from the nodes in the previous layer.
        - The input is multiplied by the weights associated with the connections between the nodes.
        - The weighted inputs are summed up, and a bias term is added.
        - The result is passed through an activation function to introduce non-linearity.
        - This process is repeated layer by layer until the final output is obtained.
- Computational Graph:

    - Definition: A computational graph is a graphical representation of the mathematical operations performed during forward and backward propagation in a neural network. Nodes in the graph represent operations, and edges represent the flow of data between these operations.
    - Nodes:
        - Input nodes: Represent the input features.
        - Operation nodes: Represent mathematical operations like multiplication, addition, etc.
        - Variable nodes: Represent intermediate values or parameters (weights and biases).
        - Output nodes: Represent the final output.
    - Edges:
Connect nodes to show the flow of data through operations.
- Back propagation:

    - Definition: Back propagation is the training algorithm for neural networks. It involves the calculation of the gradient of the loss function with respect to the weights and biases in the network. This gradient is then used to update the weights and biases to minimize the error in the predictions.
    - Steps:
        - Compute the loss between the predicted output and the actual output.
        - Propagate the error backward through the network to calculate the gradients of the loss with respect to the weights and biases.
        - Update the weights and biases using an optimization algorithm (e.g., gradient descent) to reduce the loss.
    - Chain Rule: Back propagation utilizes the chain rule of calculus to efficiently calculate gradients by decomposing the derivative of the loss with respect to a parameter into the product of derivatives along the computational graph.
  
In summary, forward propagation involves the flow of data through the network to produce predictions, a computational graph visually represents the operations involved, and back propagation is the process of adjusting the network's parameters to minimize the error in predictions. Together, these concepts form the foundation for training neural networks.

```
Inferences involve forward prop

Training involves forward prop and back prop

forward prop and back prop both involve the use of computation graphs
```