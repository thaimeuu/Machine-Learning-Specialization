## `Advanced Optimization`

### `Gradient descent`

**Gradient descent** is largely dependent on alpha (learning rate)

![Alt text](<ref img/1.1.png>)

### `Adam algorithm`

**ADAM** stands for **ADAptive Moment estimation**

**Adam** has different alpha for each weights and bias

![Alt text](<ref img/1.2.png>)

**Adam** can increase or decrease each alpha in certain situations as below in the picture:

![Alt text](<ref img/1.3.png>)

This algorithm makes **optimization (i.e. minimize the cost)** faster

### `Adam code`

![Alt text](<ref img/1.4.png>)

```python
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)
```