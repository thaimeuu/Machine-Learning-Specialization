## Gradient descent for multiple linear regression

### RECAP

![Alt text](<4. formula recap.png>)

Note: We now use ***Gradient descent*** to compute for **n** values of `w`

### NEW FORMULA

![Alt text](<4. new gradient descent formula.png>)

### AN ALTERNATIVE TO GRADIENT DESCENT

- **Normal equation**
  - Only for linear regression
  - Solve for w, b without iterations
- Disadvantages:
  - Doesn't generalize for other learning algorithms
  - Slow when number of features are large (>10000)
- What you need to know
  - Normal equation method may be used in ML libraries that implement linear regression
  - **Gradient descent** is the recommended method for finding w and b